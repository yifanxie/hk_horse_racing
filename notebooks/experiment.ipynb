{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import key sklearn ml algos\n",
    "# import key sklearn metrics\n",
    "# import \n",
    "\n",
    "from typing import Optional\n",
    "import glob\n",
    "# Add the parent directory of this notebook to sys.path\n",
    "notebook_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "parent_dir = os.path.dirname(notebook_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from project_tools import project_utils, project_class\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "# import ds_utils\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['feature_missing'] = df['feature'].isnull().astype(int)\n",
    "df['feature'].fillna(df['feature'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp_to_seconds(timestamp: str) -> float:\n",
    "    \"\"\"\n",
    "    Convert text timestamp in format \"M.SS.ss\" to total seconds\n",
    "    where M=minutes, SS=seconds, ss=decimal seconds\n",
    "    \n",
    "    Examples:\n",
    "        \"1.41.91\" -> 101.91 (1 min 41.91 sec)\n",
    "        \"1.40.12\" -> 100.12 (1 min 40.12 sec)\n",
    "        \"0.58.41\" -> 58.41 (58.41 sec)\n",
    "    \n",
    "    Args:\n",
    "        timestamp: String timestamp in M.SS.ss format\n",
    "        \n",
    "    Returns:\n",
    "        Float value representing total seconds\n",
    "    \"\"\"\n",
    "    parts = timestamp.split('.')\n",
    "    \n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Invalid timestamp format: {timestamp}. Expected format: M.SS.ss\")\n",
    "        \n",
    "    minutes = float(parts[0])\n",
    "    seconds = float(parts[1])\n",
    "    decimal = float(parts[2]) / 100  # Convert decimal part to fraction\n",
    "    \n",
    "    total_seconds = minutes * 60 + seconds + decimal\n",
    "    \n",
    "    return total_seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_to_int(date_str: str) -> int:\n",
    "    \"\"\"\n",
    "    Convert date string in YYYY-MM-DD format to integer that preserves ordering\n",
    "    \n",
    "    Args:\n",
    "        date_str: Date string in YYYY-MM-DD format\n",
    "        \n",
    "    Returns:\n",
    "        Integer in format YYYYMMDD\n",
    "        \n",
    "    Example:\n",
    "        '2015-11-18' -> 20151118b\n",
    "        '2016-03-31' -> 20160331\n",
    "    \"\"\"\n",
    "    # Remove hyphens and convert to integer\n",
    "    return int(date_str.replace('-', ''))\n",
    "\n",
    "# Example usage:\n",
    "dates = ['2015-11-18', '2015-03-25', '2016-03-31', '2015-07-05', '2016-11-06']\n",
    "date_ints = [convert_date_to_int(d) for d in dates]\n",
    "print(f\"Original dates: {dates}\")\n",
    "print(f\"Integer dates: {date_ints}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running position calculation instructions \n",
    "# given a horse race dataframe, each row provide information about a horse in a specific race,  with the following relevant columns:\n",
    "#  - horse_id - id of a horse\n",
    "# - is_winer - if the horse is a winer \n",
    "# - is_top3 - if the horse finished in top 3\n",
    "\n",
    "# the dataframe is sorted by time order with records of earlier race on top\n",
    "# write a function that calculate, and return the following for each horse at each race:\n",
    "# - running average of position of the last 3 races\n",
    "# - running average of position of the last 5 races\n",
    "# - running average of position of the last 7 races\n",
    "\n",
    "# for each results above, output a column\n",
    "\n",
    "# specific instruction:\n",
    "# - if at any given race, for a given horse, there isn't enough races to make up the running number of 3, 5 or 7, return NaN\n",
    "# - for any position with 99 - treat this as missing value \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function instruction to calculate horse prediction metrics\n",
    "with a horse racing dataframe, each row provide information about a horse in a specific race,  with the following relevant columns:\n",
    "- horse_id - id of a horse\n",
    "- is_winer - if the horse is a winer - 0 or 1\n",
    "- is_top3 - if the horse finished in top 3 - 0 or 1\n",
    "- race_id - id of a race\n",
    "- draw number - the draw number of the horse in the race\n",
    "\n",
    "now, given a list of races, for each race, given a list of predictions for each of the horse winning the race\n",
    "- ordered by the draw number, \n",
    "- the higher the value the more likely the horse is to win\n",
    "\n",
    "propose what is the best metric to evaluate the prediction quality of prediction top 1 and top 3 for a list of races\n",
    "\n",
    "\n",
    "\n",
    "do the following for each race:\n",
    "- convert the list of predictions into rank\n",
    "- check if the rank 1 (lowest value) prediction is the same as the \"is_winner\" of the horse in the race\n",
    "- check if the top rank 3 prediction is the same as the \"is_top3\" of the horse in the race\n",
    "- some race might have more than 3 top_3, i.e. more than 3 is_top3 = 1, in this case, check if the top 3 predictions contains label \"1\"\n",
    "\n",
    "return two dictionaries:\n",
    "- one for top 1 prediction:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating a machine learning model predicting horse race outcomes, particularly the winner and top 3 finishers, along with winning probabilities for each horse, you should consider several evaluation metrics. Here's a comprehensive approach to evaluate such predictions:\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "1. Log Loss: This metric is ideal for evaluating probabilistic predictions and penalizes confident misclassifications heavily[2].\n",
    "\n",
    "2. Brier Score: Similar to log loss, it measures the accuracy of probabilistic predictions.\n",
    "\n",
    "3. Accuracy: While not ideal for imbalanced datasets, it can still provide a basic measure of correctness for winner predictions[3].\n",
    "\n",
    "4. Top-K Accuracy: This is particularly useful for evaluating predictions of top 3 finishers[3].\n",
    "\n",
    "5. Mean Reciprocal Rank (MRR): This metric is valuable when you're interested in the rank of the correct prediction, especially useful for top 3 predictions.\n",
    "\n",
    "6. ROC-AUC: This can be used to evaluate the model's ability to distinguish between classes, which is useful for binary classification (e.g., whether a horse finishes in the top 3 or not)[4].\n",
    "\n",
    "## Python Implementation\n",
    "\n",
    "Here's a Python example demonstrating how to calculate these metrics:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, brier_score_loss, accuracy_score, roc_auc_score\n",
    "\n",
    "def evaluate_horse_race_predictions(y_true, y_pred_proba, top_k=3):\n",
    "    # Ensure y_true is one-hot encoded\n",
    "    y_true_onehot = np.eye(len(y_pred_proba[0]))[y_true]\n",
    "    \n",
    "    # Log Loss\n",
    "    log_loss_value = log_loss(y_true_onehot, y_pred_proba)\n",
    "    \n",
    "    # Brier Score\n",
    "    brier_score_value = brier_score_loss(y_true_onehot.ravel(), y_pred_proba.ravel())\n",
    "    \n",
    "    # Accuracy (for winner prediction)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    accuracy_value = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Top-K Accuracy\n",
    "    top_k_predictions = np.argsort(y_pred_proba, axis=1)[:, -top_k:]\n",
    "    top_k_accuracy = np.mean([1 if y_true[i] in top_k_predictions[i] else 0 for i in range(len(y_true))])\n",
    "    \n",
    "    # Mean Reciprocal Rank\n",
    "    mrr = np.mean([1 / (np.where(np.argsort(y_pred_proba[i])[::-1] == y_true[i])[0][0] + 1) for i in range(len(y_true))])\n",
    "    \n",
    "    # ROC-AUC (for top 3 prediction)\n",
    "    y_true_top3 = np.array([1 if i in top_k_predictions[j] else 0 for j, i in enumerate(y_true)])\n",
    "    y_pred_top3 = np.max(y_pred_proba, axis=1)\n",
    "    roc_auc_value = roc_auc_score(y_true_top3, y_pred_top3)\n",
    "    \n",
    "    return {\n",
    "        'Log Loss': log_loss_value,\n",
    "        'Brier Score': brier_score_value,\n",
    "        'Accuracy': accuracy_value,\n",
    "        f'Top-{top_k} Accuracy': top_k_accuracy,\n",
    "        'Mean Reciprocal Rank': mrr,\n",
    "        'ROC-AUC (Top 3)': roc_auc_value\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "y_true = np.array([2, 0, 1, 2, 1])  # True winners\n",
    "y_pred_proba = np.array([\n",
    "    [0.1, 0.3, 0.6],\n",
    "    [0.4, 0.3, 0.3],\n",
    "    [0.2, 0.5, 0.3],\n",
    "    [0.1, 0.4, 0.5],\n",
    "    [0.3, 0.4, 0.3]\n",
    "])\n",
    "\n",
    "results = evaluate_horse_race_predictions(y_true, y_pred_proba)\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "```\n",
    "\n",
    "This script calculates various metrics for evaluating horse race predictions:\n",
    "\n",
    "1. Log Loss: Evaluates the quality of probabilistic predictions.\n",
    "2. Brier Score: Measures the accuracy of probabilistic predictions.\n",
    "3. Accuracy: Calculates the proportion of correct winner predictions[3].\n",
    "4. Top-K Accuracy: Measures how often the true winner is in the top K predictions[3].\n",
    "5. Mean Reciprocal Rank: Evaluates the ranking of the correct prediction.\n",
    "6. ROC-AUC: Assesses the model's ability to distinguish between top 3 finishers and others[4].\n",
    "\n",
    "When interpreting these metrics:\n",
    "\n",
    "- Lower values are better for Log Loss and Brier Score.\n",
    "- Higher values are better for Accuracy, Top-K Accuracy, Mean Reciprocal Rank, and ROC-AUC.\n",
    "\n",
    "This comprehensive evaluation approach will give you a well-rounded assessment of your model's performance in predicting horse race outcomes, considering both the winner predictions and the probabilistic estimates for each horse.\n",
    "\n",
    "Citations:\n",
    "[1] https://github.com/dominicplouffe/HorseRacingPrediction\n",
    "[2] https://www.reddit.com/r/MachineLearning/comments/15ek21a/d_model_design_for_outputting_reliable_multiclass/\n",
    "[3] https://www.appsilon.com/post/machine-learning-evaluation-metrics-classification\n",
    "[4] https://www.kdnuggets.com/2020/04/performance-evaluation-metrics-classification.html\n",
    "[5] https://towardsdatascience.com/how-to-choose-the-best-evaluation-metric-for-classification-problems-638e845da334?gi=0b4482a290fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction evaluation when the prediction is a list of probabilities for each horse\n",
    "\n",
    "Based on your request, I'll modify the evaluation function to handle the scenario where the ground truth for each race is the finishing position of each horse. This approach allows for a more comprehensive evaluation of the model's performance in predicting the entire race outcome, not just the winner. Here's the modified function with explanations:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, ndcg_score\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "def evaluate_horse_race_positions(y_true, y_pred_proba):\n",
    "    n_races, n_horses = y_pred_proba.shape\n",
    "    \n",
    "    # Convert probabilities to predicted rankings\n",
    "    y_pred_ranks = n_horses - np.argsort(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Mean Squared Error\n",
    "    mse = mean_squared_error(y_true, y_pred_ranks)\n",
    "    \n",
    "    # Mean Absolute Error\n",
    "    mae = mean_absolute_error(y_true, y_pred_ranks)\n",
    "    \n",
    "    # Spearman's Rank Correlation\n",
    "    spearman_corr = np.mean([kendalltau(y_true[i], y_pred_ranks[i]).correlation \n",
    "                             for i in range(n_races)])\n",
    "    \n",
    "    # Normalized Discounted Cumulative Gain (NDCG)\n",
    "    ndcg = ndcg_score(y_true.reshape(1, -1), y_pred_proba.reshape(1, -1))\n",
    "    \n",
    "    # Exact Match Accuracy\n",
    "    exact_match = np.mean(np.all(y_true == y_pred_ranks, axis=1))\n",
    "    \n",
    "    # Top-K Accuracy (for K=1, 2, 3)\n",
    "    top_k_accuracy = {}\n",
    "    for k in [1, 2, 3]:\n",
    "        top_k_pred = np.argsort(-y_pred_proba, axis=1)[:, :k]\n",
    "        top_k_true = np.argsort(y_true, axis=1)[:, :k]\n",
    "        top_k_accuracy[f'Top-{k} Accuracy'] = np.mean([\n",
    "            len(set(top_k_pred[i]) & set(top_k_true[i])) / k \n",
    "            for i in range(n_races)\n",
    "        ])\n",
    "    \n",
    "    return {\n",
    "        'Mean Squared Error': mse,\n",
    "        'Mean Absolute Error': mae,\n",
    "        \"Spearman's Rank Correlation\": spearman_corr,\n",
    "        'NDCG': ndcg,\n",
    "        'Exact Match Accuracy': exact_match,\n",
    "        **top_k_accuracy\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "y_true = np.array([\n",
    "    [4, 1, 3, 2, 5],\n",
    "    [2, 5, 1, 4, 3],\n",
    "    [3, 1, 5, 2, 4]\n",
    "])\n",
    "\n",
    "y_pred_proba = np.array([\n",
    "    [0.1, 0.4, 0.2, 0.2, 0.1],\n",
    "    [0.3, 0.1, 0.4, 0.1, 0.1],\n",
    "    [0.2, 0.3, 0.1, 0.3, 0.1]\n",
    "])\n",
    "\n",
    "results = evaluate_horse_race_positions(y_true, y_pred_proba)\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "```\n",
    "\n",
    "This modified function evaluates the predictions based on the full finishing order of each race. Here's an explanation of the metrics used:\n",
    "\n",
    "1. **Mean Squared Error (MSE)**: Measures the average squared difference between predicted and actual rankings.\n",
    "\n",
    "2. **Mean Absolute Error (MAE)**: Measures the average absolute difference between predicted and actual rankings.\n",
    "\n",
    "3. **Spearman's Rank Correlation**: Assesses the monotonic relationship between predicted and actual rankings. A higher value indicates better prediction of the overall order.\n",
    "\n",
    "4. **Normalized Discounted Cumulative Gain (NDCG)**: Measures the quality of ranking, giving more importance to correct predictions at higher ranks.\n",
    "\n",
    "5. **Exact Match Accuracy**: Calculates the proportion of races where the predicted order exactly matches the true order.\n",
    "\n",
    "6. **Top-K Accuracy**: For K=1, 2, and 3, it measures how well the model predicts the top K finishers, regardless of their exact order.\n",
    "\n",
    "This evaluation approach provides a comprehensive assessment of the model's performance in predicting horse race outcomes, considering the entire finishing order rather than just the winner or top few positions. It captures various aspects of ranking quality, from exact matches to overall correlation of rankings.\n",
    "\n",
    "To use this function, provide the true finishing positions (`y_true`) and the predicted probabilities (`y_pred_proba`) for each horse in each race. The function will return a dictionary of evaluation metrics that you can use to assess and compare different models or prediction approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = {}\n",
    "ground_truth = {}\n",
    "even_probs = []\n",
    "random_probs = []\n",
    "winning_odd_preds = [] \n",
    "\n",
    "for race in val_df['race_id'].unique():\n",
    "    race_df = val_df[val_df['race_id']==race]\n",
    "    n_horse = race_df.shape[0]    \n",
    "    eval_dict[race] = {}\n",
    "    eval_dict[race]['ground_truth'] = race_df['clean_position'].values\n",
    "    eval_dict[race]['even_probs'] = np.array([1/n_horse] * n_horse)\n",
    "    eval_dict[race]['random_probs'] = np.random.random(n_horse)\n",
    "    eval_dict[race]['winning_odd_preds'] = race_df['clean_win_odds'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction_sets(eval_dict):\n",
    "    \"\"\"\n",
    "    Evaluate different prediction sets against ground truth for each race and calculate mean metrics\n",
    "    \n",
    "    Args:\n",
    "        eval_dict: Dictionary containing race data with ground truth and different prediction sets\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (eval_result, mean_results_df)\n",
    "            - eval_result: Dictionary with detailed evaluation metrics for each race\n",
    "            - mean_results_df: DataFrame comparing mean metrics across prediction types\n",
    "    \"\"\"\n",
    "    # Initialize results dictionary\n",
    "    eval_result = {\n",
    "        'even_probs': [],\n",
    "        'random_probs': [], \n",
    "        'winning_odd_preds': []\n",
    "    }\n",
    "\n",
    "    # Initialize dictionaries to store mean results\n",
    "    mean_results = {\n",
    "        'even_probs': {},\n",
    "        'random_probs': {},\n",
    "        'winning_odd_preds': {}\n",
    "    }\n",
    "\n",
    "    # Loop through each race\n",
    "    for race_id in eval_dict:\n",
    "        race_data = eval_dict[race_id]\n",
    "        ground_truth = race_data['ground_truth'].reshape(1,-1)\n",
    "        \n",
    "        # Evaluate each prediction type\n",
    "        for pred_type in ['even_probs', 'random_probs', 'winning_odd_preds']:\n",
    "            pred_probs = race_data[pred_type].reshape(1,-1)\n",
    "            \n",
    "            # Evaluate predictions for this race\n",
    "            race_eval = evaluate_horse_race_positions(\n",
    "                ground_truth,\n",
    "                pred_probs.reshape(1,-1)\n",
    "            )\n",
    "            \n",
    "            # Store results for this race\n",
    "            eval_result[pred_type].append(race_eval)\n",
    "\n",
    "    # Calculate mean results for each prediction type\n",
    "    for pred_type in ['even_probs', 'random_probs', 'winning_odd_preds']:\n",
    "        # Initialize dict to store means for each metric\n",
    "        metric_means = {}\n",
    "        \n",
    "        # Get all metrics from first result to know what to average\n",
    "        metrics = eval_result[pred_type][0].keys()\n",
    "        \n",
    "        # Calculate mean for each metric\n",
    "        for metric in metrics:\n",
    "            metric_values = [result[metric] for result in eval_result[pred_type]]\n",
    "            metric_means[metric] = np.mean(metric_values)\n",
    "        \n",
    "        mean_results[pred_type] = metric_means\n",
    "    \n",
    "    # Convert mean results to DataFrame for easy comparison\n",
    "    mean_results_df = pd.DataFrame(mean_results)\n",
    "    \n",
    "    return eval_result, mean_results_df\n",
    "\n",
    "# Run evaluation\n",
    "eval_result, mean_results_df = evaluate_prediction_sets(eval_dict)\n",
    "\n",
    "# Display mean results comparison\n",
    "print(\"\\nMean Evaluation Metrics Comparison:\")\n",
    "print(mean_results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lightgbm training function prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
